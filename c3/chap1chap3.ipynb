{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mes fichiers de scrapping du chapitre 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la page web à scraper\n",
    "url = \"https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset\" \n",
    "# Effectuer la requête GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête s'est bien passée\n",
    "if response.status_code == 200:\n",
    "    # Parser le contenu HTML de la page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Trouver tous les tableaux (par exemple, tableau avec class 'medical-data')\n",
    "    tables = soup.find_all('table', class_='medical-data')  # Adapte la classe ou tag à la page cible\n",
    "\n",
    "    # Extraire les données du tableau\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        # Parcourir chaque ligne du tableau\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            data = [col.text.strip() for col in cols]  # Nettoyer les espaces inutiles\n",
    "            print(data)\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête, code HTTP: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset\n",
      "Le téléchargement du jeu de données est terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Définir les variables d'environnement avec vos identifiants Kaggle\n",
    "os.environ['KAGGLE_USERNAME'] = 'yvespaulito'\n",
    "os.environ['KAGGLE_KEY'] = '63e95328af24675ff1902fa4362480a9'\n",
    "\n",
    "# Créer une instance de l'API Kaggle\n",
    "api = KaggleApi()\n",
    "\n",
    "# Authentifier l'API Kaggle en utilisant les variables d'environnement\n",
    "api.authenticate()\n",
    "\n",
    "# Télécharger le jeu de données spécifié\n",
    "dataset_slug = \"uom190346a/sleep-health-and-lifestyle-dataset\"\n",
    "\n",
    "try:\n",
    "    # Télécharger et décompresser le jeu de données\n",
    "    api.dataset_download_files(dataset_slug, path='.', unzip=True)\n",
    "    print(\"Le téléchargement du jeu de données est terminé.\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur s'est produite : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset extracted successfully.\n",
      "Files in ZIP:\n",
      "File Name                                             Modified             Size\n",
      "healthcare_dataset.csv                         2024-05-08 12:15:18      8399221\n",
      "Files in extracted folder:\n",
      "Found file: healthcare_dataset.csv\n",
      "            Name  Age  Gender Blood Type Medical Condition Date of Admission  \\\n",
      "0  Bobby JacksOn   30    Male         B-            Cancer        2024-01-31   \n",
      "1   LesLie TErRy   62    Male         A+           Obesity        2019-08-20   \n",
      "2    DaNnY sMitH   76  Female         A-           Obesity        2022-09-22   \n",
      "3   andrEw waTtS   28  Female         O+          Diabetes        2020-11-18   \n",
      "4  adrIENNE bEll   43  Female        AB+            Cancer        2022-09-19   \n",
      "\n",
      "             Doctor                    Hospital Insurance Provider  \\\n",
      "0     Matthew Smith             Sons and Miller         Blue Cross   \n",
      "1   Samantha Davies                     Kim Inc           Medicare   \n",
      "2  Tiffany Mitchell                    Cook PLC              Aetna   \n",
      "3       Kevin Wells  Hernandez Rogers and Vang,           Medicare   \n",
      "4    Kathleen Hanna                 White-White              Aetna   \n",
      "\n",
      "   Billing Amount  Room Number Admission Type Discharge Date   Medication  \\\n",
      "0    18856.281306          328         Urgent     2024-02-02  Paracetamol   \n",
      "1    33643.327287          265      Emergency     2019-08-26    Ibuprofen   \n",
      "2    27955.096079          205      Emergency     2022-10-07      Aspirin   \n",
      "3    37909.782410          450       Elective     2020-12-18    Ibuprofen   \n",
      "4    14238.317814          458         Urgent     2022-10-09   Penicillin   \n",
      "\n",
      "   Test Results  \n",
      "0        Normal  \n",
      "1  Inconclusive  \n",
      "2        Normal  \n",
      "3      Abnormal  \n",
      "4      Abnormal  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Configuration\n",
    "KAGGLE_USERNAME = 'yvespaulito'  \n",
    "KAGGLE_KEY = '63e95328af24675ff1902fa4362480a9' \n",
    "DATASET_NAME = 'prasad22/healthcare-dataset'\n",
    "ZIP_FILE_NAME = 'dataset.zip'\n",
    "EXTRACTED_FOLDER = 'dataset'\n",
    "CSV_FILE_NAME = 'healthcare_dataset.csv'  # Nom corrigé du fichier CSV\n",
    "\n",
    "# Télécharger le dataset\n",
    "def download_dataset():\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {KAGGLE_KEY}'\n",
    "    }\n",
    "    url = f'https://www.kaggle.com/api/v1/datasets/download/{DATASET_NAME}'\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(ZIP_FILE_NAME, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(\"Dataset downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Extraire le fichier ZIP\n",
    "def extract_zip():\n",
    "    with zipfile.ZipFile(ZIP_FILE_NAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACTED_FOLDER)\n",
    "        print(\"Dataset extracted successfully.\")\n",
    "        # Liste le contenu du fichier ZIP pour identifier le fichier CSV\n",
    "        print(\"Files in ZIP:\")\n",
    "        zip_ref.printdir()\n",
    "\n",
    "# Lire le fichier CSV\n",
    "def read_csv():\n",
    "    # Liste le contenu du dossier extrait pour identifier le fichier CSV\n",
    "    print(\"Files in extracted folder:\")\n",
    "    for root, dirs, files in os.walk(EXTRACTED_FOLDER):\n",
    "        for file in files:\n",
    "            print(f\"Found file: {file}\")\n",
    "\n",
    "    # Utilisation du nom correct du fichier CSV extrait\n",
    "    file_path = os.path.join(EXTRACTED_FOLDER, CSV_FILE_NAME)\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Exécution des fonctions\n",
    "download_dataset()\n",
    "extract_zip()\n",
    "df = read_csv()\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "#Le fichier ZIP contenant le dataset est téléchargé depuis Kaggle.\n",
    "\n",
    "#Le contenu du ZIP est décompressé dans le dossier dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page récupérée avec succès\n",
      "Données sauvegardées dans 'scraped_sleep_data_full_page.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL de la page à scraper\n",
    "url = \"https://www.sleepfoundation.org/how-sleep-works\"\n",
    "\n",
    "# Définir des en-têtes pour imiter une requête d'un navigateur\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Envoi de la requête HTTP avec les en-têtes\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Vérifie si la page a été récupérée avec succès\n",
    "if response.status_code == 200:\n",
    "    print(\"Page récupérée avec succès\")\n",
    "    \n",
    "    # Utilisation de BeautifulSoup pour analyser le contenu HTML de la page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Récupérer tous les titres h1, h2, h3 et les paragraphes\n",
    "    titles_h1 = soup.find_all('h1')\n",
    "    titles_h2 = soup.find_all('h2')\n",
    "    titles_h3 = soup.find_all('h3')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    # Récupérer les sections de la page qui contiennent des informations détaillées\n",
    "    sections = soup.find_all('section')  # Cela récupère les sections principales de la page\n",
    "    \n",
    "    # Récupérer tous les liens\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Récupérer toutes les images\n",
    "    images = soup.find_all('img', src=True)\n",
    "\n",
    "    # Créer un fichier CSV pour sauvegarder les données\n",
    "    with open('scraped_sleep_data_full_page.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Écrire l'en-tête du CSV\n",
    "        writer.writerow(['Type', 'Content'])\n",
    "\n",
    "        # Ajouter les titres h1, h2, h3 dans le CSV\n",
    "        for title in titles_h1:\n",
    "            writer.writerow(['H1', title.text.strip()])\n",
    "        for title in titles_h2:\n",
    "            writer.writerow(['H2', title.text.strip()])\n",
    "        for title in titles_h3:\n",
    "            writer.writerow(['H3', title.text.strip()])\n",
    "\n",
    "        # Ajouter les paragraphes dans le CSV\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.text.strip()\n",
    "            if text:  # Ne pas ajouter de lignes vides\n",
    "                writer.writerow(['P', text])\n",
    "\n",
    "        # Ajouter les sections dans le CSV (par exemple, sous-sections détaillées)\n",
    "        for section in sections:\n",
    "            section_text = section.get_text(separator=' ', strip=True)\n",
    "            if section_text:  # Ne pas ajouter de sections vides\n",
    "                writer.writerow(['Section', section_text])\n",
    "\n",
    "        # Ajouter les liens dans le CSV\n",
    "        for link in links:\n",
    "            writer.writerow(['Link', link['href']])\n",
    "\n",
    "        # Ajouter les images dans le CSV\n",
    "        for image in images:\n",
    "            writer.writerow(['Image', image['src']])\n",
    "\n",
    "    print(\"Données sauvegardées dans 'scraped_sleep_data_full_page.csv'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Erreur lors du téléchargement de la page : {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
